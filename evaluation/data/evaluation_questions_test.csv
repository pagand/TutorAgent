"question_number","question_text","question_type","options","correct_answer","skill_id","difficulty","gold_standard_hint"
"1","What is the primary mechanism LoRA uses to adapt a frozen transformer weight matrix W?","multiple_choice","It fine-tunes the entire W matrix|It inserts a low-rank adapter AB while keeping W frozen|It replaces W with a quantized approximation|It prunes W to fewer parameters","It inserts a low-rank adapter AB while keeping W frozen","[LoRA-Quantization-Basics]","easy","Recall the decomposition described in the document: the new weight is written as W + ΔW, where ΔW is expressed in a low-rank factorization (A and B). "
"2","In QLoRA the base model is commonly quantized to how many bits in the example given in the document?","fill_in_the_blank","","4","[LoRA-Quantization-Basics]","easy","The introduction gives an explicit example of QLoRA using a very low-bit base model; remember the bit-width example used. "
"3","What symbol does the document use to denote the quantization error between the quantized weight and the original full-precision weight?","multiple_choice","δ (delta)|ε (epsilon)|η (eta)|κ (kappa)","ε (epsilon)","[LoRA-Quantization-Basics-2]","easy","The core theory section writes the quantized weight as Ŵ = W + ? and names that deviation with a single Greek letter. "