"question_number","question_text","question_type","options","correct_answer","skill_id","difficulty","gold_standard_hint", "source_reference"
"1","What is the primary mechanism LoRA uses to adapt a frozen transformer weight matrix W?","multiple_choice","It fine-tunes the entire W matrix|It inserts a low-rank adapter AB while keeping W frozen|It replaces W with a quantized approximation|It prunes W to fewer parameters","It inserts a low-rank adapter AB while keeping W frozen","[LoRA-Quantization-Basics]","easy","Recall the decomposition described in the document: the new weight is written as W + ΔW, where ΔW is expressed in a low-rank factorization (A and B). ", "In LoRA, for a transformer linear weight (W ...), one inserts a low-rank decomposition: W_new = W + ΔW, ΔW = A B ... Only (A, B) are trainable; (W) is frozen."
"2","In QLoRA the base model is commonly quantized to how many bits?","fill_in_the_blank","","4","[LoRA-Quantization-Basics]","easy","The introduction gives an explicit example of QLoRA using a very low-bit base model; remember the bit-width example used. ", "QLoRA extends this by quantizing the base model (e.g. to 4 bits) and keeping it frozen, while training LoRA adapters in higher precision"
"3","How can significant quantization error ε negatively affect LoRA adapter training?","multiple_choice","It speeds up convergence incorrectly|It makes the adapter learn to cancel quantization noise instead of the true task adaptation|It allows training of the full base weights|It makes the model immune to overfitting","It makes the adapter learn to cancel quantization noise instead of the true task adaptation","[LoRA-Quantization-Basics]","hard","Think about how a persistent offset in the base weights shifts activations and thus the gradient landscape the adapter sees. The document describes this exact failure mode. ","in worst case the adapter learns to cancel quantization noise rather than learn the true task adaptation."
"4","What is the main purpose of QuAILoRA as described?","multiple_choice","To quantize adapters to lower precision|To initialize LoRA adapters to compensate for quantization error|To replace LoRA with a different adapter type|To fine-tune full model weights after quantization","To initialize LoRA adapters to compensate for quantization error","[QuAILoRA-Initialization]","easy","The introduction contrasts QLoRA and QuAILoRA and states QuAILoRA's role relative to quantization error at initialization. ", "QuAILoRA is a technique that initializes the LoRA adapter weights in a way that is aware of the quantization error, reducing the negative effects"
"5","Write the initialization formulas for A_init and B_init that QuAILoRA proposes (use U_r, Σ_r, V_r notation).","fill_in_the_blank","","A_init = - U_r Σ_r^{1/2}, B_init = Σ_r^{1/2} V_r^T","[QuAILoRA-Initialization]","hard","The initialization step projects ε into a rank-r SVD and then sets A and B so their product approximates -ε in that low-rank subspace. Recall the exact square-root splitting used. ", "A_init = - U_r Σ_r^{1/2}, B_init = Σ_r^{1/2} V_r^T"
"6","a practical way to estimate the quantization residual ε is to:","multiple_choice","Assume ε is zero and skip estimation|Compute ε analytically without data|Use a small calibration dataset to compare quantized vs original weights|Train a separate model to predict ε","Use a small calibration dataset to compare quantized vs original weights","[QuAILoRA-Initialization]","hard","The paper suggests an inexpensive calibration step running a few batches through both models to form an empirical estimate of the weight residual. ", "In practice, one can approximate (ε) using calibration with a small data sample (e.g. run a few batches through the quantized model vs original to record weight differences)"
"7","QuAILoRA initializes A and B so that A B is approximately equal to which of the following within the low-rank span?","multiple_choice","+W|+Ŵ|-ε|-Ŵ","-ε","[QuAILoRA-Initialization]","hard","Recall the core correction idea: the adapter is biased to initially 'undo' the quantization error in the dominant low-rank directions. ", "The core idea: ... initialize the adapter so that: AB≈-ε in the subspace that (ε) lies in (within the low-rank span)"
"8","What does 'group-wise' refer to in the QA-LoRA implementation strategy?","multiple_choice","Grouping training examples by label|Partitioning a weight matrix into sub-blocks and applying quantization+LoRA per group|Grouping models by architecture|Grouping optimizers by learning rate","Partitioning a weight matrix into sub-blocks and applying quantization+LoRA per group","[QA-LoRA-Implementation]","easy","The core mechanism section explains splitting W into groups and handling quantization parameters and adapters per submatrix. ", "One of the core ideas: treat quantization and adaptation jointly per group (i.e. partition the weight matrix into sub-blocks or groups). For each group, you maintain quantization parameters and LoRA parameters."
"9","In the grouped forward pass pseudocode, the group's contribution to the output is computed as (fill with the expression shown):","fill_in_the_blank","","x @ Wqg.T + x @ (B_i.T @ A_i.T)","[QA-LoRA-Implementation]","hard","The forward override explicitly shows the base quantized multiply plus the group's LoRA contribution; recall the exact matrix-orientation used in the pseudo-code. ", "out = x @ Wqg.T # base ... out += x @ (B_i.T @ A_i.T) # or depending how you orient A, B"
"10","After training adapters per group, how does the document show the final merged quantized weight for inference is formed?","multiple_choice","Wqg_merged = A_i @ B_i|Wqg_merged = Wqg - (A_i @ B_i)|Wqg_merged = Wqg + (A_i @ B_i)|Wqg_merged = Wqg * (A_i @ B_i)","Wqg_merged = Wqg + (A_i @ B_i)","[QA-LoRA-Implementation]","hard","Consider the merge/export step: adapters are folded into the quantized group weights so no extra float conversion is required at inference. ", ""
"11","Which of the following is NOT listed as a key hyperparameter or interface choice for QA-LoRA in the document?","multiple_choice","Group size / grouping dimension|Rank per group|Quantization bit width|Choice of activation function for the base model","Choice of activation function for the base model","[QA-LoRA-Implementation]","hard","Review the 'Key Hyperparameters & Interface Changes' list—most entries concern grouping, rank, and quantization bits rather than base activation functions. ", ""
"12","One practical limitation of QuAILoRA noted in the document is:","multiple_choice","It eliminates the need for any calibration data|You must have access to the full-precision weights or their difference versus quantized weights|It reduces validation perplexity in all cases|It removes the compute cost of SVD entirely","You must have access to the full-precision weights or their difference versus quantized weights","[Evaluation-Limitations]","easy","Check the limitations section—access to full-precision weights (or their difference) is explicitly called out as a potential barrier. ",""
"13","When is the QuAILoRA initialization likely to offer minimal benefit, per the document?","multiple_choice","When quantization error is dominated by a low-rank component|When the adapter rank is large enough|When quantization error is very small or random high-frequency noise|When you can access full-precision weights easily","When quantization error is very small or random high-frequency noise","[Evaluation-Limitations]","hard","The limitations describe cases where ε is small or high-frequency/random — in those cases the initialization step may not meaningfully help. ",""
"14","QuAILoRA's empirical benefit on benchmarks is most clearly observed as a reduction in validation _____ compared to plain QLoRA.","fill_in_the_blank","","perplexity","[Evaluation-Limitations]","hard","The empirical benefits section mentions that QuAILoRA tends to reduce validation metric X relative to plain QLoRA; recall that specific NLP evaluation metric used. ",""
"15","The QuAILoRA initialization assumes the quantization residual ε can be well-approximated by which structural property?","multiple_choice","High-rank random noise|Low-rank dominant component|Sparse binary mask|Stationary Gaussian noise","Low-rank dominant component","[Evaluation-Limitations]","hard","The correction method projects ε into a rank-r SVD; the assumption is that dominant modes are captured by a low-rank approximation. ", ""
